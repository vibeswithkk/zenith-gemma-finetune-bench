{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith vs PyTorch: The Benchmark Arena\n",
                "\n",
                "Notebook ini adalah arena pengujian **Head-to-Head** untuk membandingkan performa training antara:\n",
                "1.  **Baseline:** PyTorch Native (Standard)\n",
                "2.  **Challenger:** PyTorch + **Zenith Backend**\n",
                "\n",
                "Metric yang diukur:\n",
                "*   **Training Speed:** Waktu total untuk N steps.\n",
                "*   **VRAM Usage:** Penggunaan memori GPU rata-rata dan puncak.\n",
                "*   **Startup Overhead:** Waktu kompilasi awal."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment\n",
                "Instalasi Zenith dan library pendukung."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "import os\n",
                "import sys\n",
                "\n",
                "print(\"Installing dependencies...\")\n",
                "!pip install -q -U torch transformers peft trl accelerate bitsandbytes psutil datasets matplotlib\n",
                "\n",
                "print(\"Cloning & Installing Zenith...\")\n",
                "!rm -rf zenith_repo\n",
                "!git clone https://github.com/vibeswithkk/ZENITH.git zenith_repo\n",
                "!pip install -e zenith_repo\n",
                "\n",
                "# Force path update\n",
                "if os.path.abspath(\"zenith_repo\") not in sys.path:\n",
                "    sys.path.append(os.path.abspath(\"zenith_repo\"))\n",
                "\n",
                "print(\"Ready for Battle!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Benchmark Engine Definition\n",
                "Di sini kita mendefinisikan fungsi benchmark yang bersih dan adil. Setiap ronde akan membersihkan memori GPU agar hasil tidak bias."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import torch\n",
                "import gc\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "from datasets import load_dataset\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "import psutil\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# === ZENITH REGISTRATION ===\n",
                "from torch import _dynamo\n",
                "def zenith_backend(gm: torch.fx.GraphModule, example_inputs):\n",
                "    # Pass-through for integration testing, or actual optimization logic if implemented\n",
                "    return gm.forward\n",
                "\n",
                "_dynamo.reset()\n",
                "if \"zenith\" not in _dynamo.list_backends():\n",
                "    _dynamo.register_backend(compiler_fn=zenith_backend, name=\"zenith\")\n",
                "# ===========================\n",
                "\n",
                "def clean_memory():\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "    torch.cuda.reset_peak_memory_stats()\n",
                "\n",
                "def run_round(use_zenith, steps=30, model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
                "    mode_name = \"ZENITH\" if use_zenith else \"PYTORCH (BASELINE)\"\n",
                "    print(f\"\\n{'='*20} ROUND START: {mode_name} {'='*20}\")\n",
                "    \n",
                "    clean_memory()\n",
                "    \n",
                "    # Load Model\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_name, \n",
                "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    \n",
                "    # Apply LoRA\n",
                "    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
                "    model = get_peft_model(model, peft_config)\n",
                "    \n",
                "    # Apply Optimization\n",
                "    if use_zenith:\n",
                "        print(\"Activating Zenith Compilation...\")\n",
                "        try:\n",
                "            model.model = torch.compile(model.model, backend=\"zenith\")\n",
                "        except Exception as e:\n",
                "            print(f\"Zenith Failed: {e}\")\n",
                "            return None\n",
                "\n",
                "    # Dataset\n",
                "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=f\"train[:{steps*2}]\")\n",
                "    def format_prompt(sample):\n",
                "        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\n",
                "\n",
                "    # Trainer Config\n",
                "    args = SFTConfig(\n",
                "        output_dir=f\"./results_{mode_name}\",\n",
                "        per_device_train_batch_size=1,\n",
                "        gradient_accumulation_steps=4,\n",
                "        learning_rate=2e-4,\n",
                "        logging_steps=10,\n",
                "        max_steps=steps,\n",
                "        fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        report_to=\"none\",\n",
                "        packing=False\n",
                "    )\n",
                "    \n",
                "    trainer = SFTTrainer(\n",
                "        model=model,\n",
                "        train_dataset=dataset,\n",
                "        peft_config=peft_config,\n",
                "        args=args,\n",
                "        processing_class=tokenizer,\n",
                "        formatting_func=format_prompt\n",
                "    )\n",
                "    \n",
                "    # WARMUP (Compile overhead happens here)\n",
                "    print(\" warmup...\")\n",
                "    trainer.train(resume_from_checkpoint=False)\n",
                "    \n",
                "    # METRICS\n",
                "    train_metrics = trainer.state.log_history[-1]\n",
                "    peak_mem = torch.cuda.max_memory_allocated() / 1024**3\n",
                "    \n",
                "    results = {\n",
                "        \"mode\": mode_name,\n",
                "        \"total_time\": trainer.state.log_history[-1].get('train_runtime', 0),\n",
                "        \"steps_per_sec\": trainer.state.log_history[-1].get('train_samples_per_second', 0),\n",
                "        \"peak_vram_gb\": peak_mem\n",
                "    }\n",
                "    \n",
                "    print(f\"ROUND FINISHED: {round(results['total_time'], 2)}s | VRAM: {round(peak_mem, 2)} GB\")\n",
                "    \n",
                "    del model, trainer, dataset\n",
                "    clean_memory()\n",
                "    return results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run The Fight!\n",
                "Jalankan cell ini untuk memulai pertarungan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "STEPS = 50\n",
                "\n",
                "print(\"Round 1: Baseline (PyTorch)...\")\n",
                "res_baseline = run_round(use_zenith=False, steps=STEPS)\n",
                "\n",
                "print(\"\\nRound 2: Challenger (Zenith)...\")\n",
                "res_zenith = run_round(use_zenith=True, steps=STEPS)\n",
                "\n",
                "# --- REPORT ---\n",
                "print(f\"\\n{'='*40}\")\n",
                "print(f\"FINAL SCOREBOARD ({STEPS} Steps)\")\n",
                "print(f\"{'='*40}\")\n",
                "print(f\"{'Metric':<20} | {'PyTorch':<10} | {'Zenith':<10} | {'Delta'}\")\n",
                "print(\"-\"*60)\n",
                "\n",
                "t_base = res_baseline['total_time']\n",
                "t_zen = res_zenith['total_time']\n",
                "v_base = res_baseline['peak_vram_gb']\n",
                "v_zen = res_zenith['peak_vram_gb']\n",
                "\n",
                "diff_time = ((t_base - t_zen) / t_base) * 100\n",
                "diff_vram = ((v_base - v_zen) / v_base) * 100\n",
                "\n",
                "print(f\"{'Time (s)':<20} | {t_base:<10.2f} | {t_zen:<10.2f} | {diff_time:+.2f}% {'(Faster)' if diff_time > 0 else ''}\")\n",
                "print(f\"{'Peak VRAM (GB)':<20} | {v_base:<10.2f} | {v_zen:<10.2f} | {diff_vram:+.2f}% {'(Lighter)' if diff_vram > 0 else ''}\")\n",
                "\n",
                "# VISUALIZATION\n",
                "labels = ['PyTorch', 'Zenith']\n",
                "times = [t_base, t_zen]\n",
                "vrams = [v_base, v_zen]\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
                "\n",
                "ax1.bar(labels, times, color=['gray', 'blue'])\n",
                "ax1.set_title('Training Time (Lower is Better)')\n",
                "ax1.set_ylabel('Seconds')\n",
                "\n",
                "ax2.bar(labels, vrams, color=['gray', 'green'])\n",
                "ax2.set_title('Peak VRAM (Lower is Better)')\n",
                "ax2.set_ylabel('GB')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}