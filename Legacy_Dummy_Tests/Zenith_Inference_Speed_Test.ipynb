{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  Zenith Inference Speed Test: Tokens Per Second (TPS)\n",
                "\n",
                "Notebook ini dirancang untuk mengukur kecepatan inferensi (generation speed) antara:\n",
                "1.  **Baseline:** PyTorch Native `model.generate()`\n",
                "2.  **Zenith:** Optimized `torch.compile(model, backend='zenith')`\n",
                "\n",
                "Metrics:\n",
                "*   **Time to First Token (TTFT):** Latensi awal.\n",
                "*   **Tokens Per Second (TPS):** Kecepatan total output teks.\n",
                "*   **Total Inference Time:** Waktu keseluruhan."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "import os\n",
                "import sys\n",
                "\n",
                "print(\"Installing dependencies...\")\n",
                "!pip install -q -U torch transformers accelerate bitsandbytes psutil matplotlib\n",
                "\n",
                "print(\"Cloning & Installing Zenith...\")\n",
                "!rm -rf zenith_repo\n",
                "!git clone https://github.com/vibeswithkk/ZENITH.git zenith_repo\n",
                "!pip install -e zenith_repo\n",
                "\n",
                "if os.path.abspath(\"zenith_repo\") not in sys.path:\n",
                "    sys.path.append(os.path.abspath(\"zenith_repo\"))\n",
                "\n",
                "import torch\n",
                "from torch import _dynamo\n",
                "\n",
                "# Register Dummy Backend if not present (for test without full kernel build)\n",
                "def zenith_backend(gm: torch.fx.GraphModule, example_inputs):\n",
                "    return gm.forward\n",
                "\n",
                "_dynamo.reset()\n",
                "if \"zenith\" not in _dynamo.list_backends():\n",
                "    _dynamo.register_backend(compiler_fn=zenith_backend, name=\"zenith\")\n",
                "\n",
                "print(\"Ready for Inference Testing!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import gc\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "PROMPT = \"The future of Artificial Intelligence is\"\n",
                "MAX_NEW_TOKENS = 100\n",
                "\n",
                "def load_model():\n",
                "    print(f\"Loading {MODEL_ID}...\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_ID, \n",
                "        torch_dtype=torch.float16, \n",
                "        device_map=\"cuda\"\n",
                "    )\n",
                "    return model, tokenizer\n",
                "\n",
                "def benchmark_inference(model, tokenizer, use_zenith=False, runs=5):\n",
                "    mode = \"ZENITH\" if use_zenith else \"PYTORCH\"\n",
                "    print(f\"\\n{'='*10} BENCHMARK: {mode} {'='*10}\")\n",
                "    \n",
                "    if use_zenith:\n",
                "        print(\"Compiling model with Zenith backend...\")\n",
                "        # Compile the forward pass of the model\n",
                "        model = torch.compile(model, backend=\"zenith\")\n",
                "    \n",
                "    input_ids = tokenizer(PROMPT, return_tensors=\"pt\").input_ids.cuda()\n",
                "    \n",
                "    # Warmup\n",
                "    print(\"Warming up... (This compiles the graph if Zenith is on)\")\n",
                "    _ = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n",
                "    \n",
                "    latencies = []\n",
                "    tokens_per_sec = []\n",
                "    \n",
                "    print(f\"Running {runs} generations...\")\n",
                "    for i in range(runs):\n",
                "        torch.cuda.synchronize()\n",
                "        start_time = time.time()\n",
                "        \n",
                "        output = model.generate(input_ids, max_new_tokens=MAX_NEW_TOKENS, do_sample=False)\n",
                "        \n",
                "        torch.cuda.synchronize()\n",
                "        end_time = time.time()\n",
                "        \n",
                "        latency = end_time - start_time\n",
                "        num_tokens = len(output[0]) - len(input_ids[0])\n",
                "        tps = num_tokens / latency\n",
                "        \n",
                "        latencies.append(latency)\n",
                "        tokens_per_sec.append(tps)\n",
                "        print(f\"Running {i+1}: {tps:.2f} tokens/sec ({latency:.4f}s)\")\n",
                "        \n",
                "    avg_tps = np.mean(tokens_per_sec)\n",
                "    print(f\"AVG TPS ({mode}): {avg_tps:.2f}\")\n",
                "    \n",
                "    # Cleanup\n",
                "    del output\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "    \n",
                "    return avg_tps, tokens_per_sec, model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Run Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Load Baseline Model\n",
                "model, tokenizer = load_model()\n",
                "\n",
                "# 2. Benchmark PyTorch\n",
                "tps_baseline, _, model = benchmark_inference(model, tokenizer, use_zenith=False)\n",
                "\n",
                "# 3. Benchmark Zenith (Compile SAME model)\n",
                "# Note: In real scenarios, we might reload ensuring clean slate, but compile usually handles inplace optimization\n",
                "tps_zenith, _, _ = benchmark_inference(model, tokenizer, use_zenith=True)\n",
                "\n",
                "# 4. Results\n",
                "print(f\"\\n{'='*40}\")\n",
                "print(f\"INFERENCE SCOREBOARD\")\n",
                "print(f\"{'='*40}\")\n",
                "print(f\"PyTorch: {tps_baseline:.2f} TPS\")\n",
                "print(f\"Zenith : {tps_zenith:.2f} TPS\")\n",
                "delta = ((tps_zenith - tps_baseline) / tps_baseline) * 100\n",
                "print(f\"Improvement: {delta:+.2f}%\")\n",
                "\n",
                "# Plot\n",
                "labels = ['PyTorch', 'Zenith']\n",
                "values = [tps_baseline, tps_zenith]\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "bars = plt.bar(labels, values, color=['gray', 'blue'])\n",
                "plt.title('Inference Speed (Tokens Per Second) - Higher is Better')\n",
                "plt.ylabel('TPS')\n",
                "\n",
                "for bar in bars:\n",
                "    yval = bar.get_height()\n",
                "    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
                "\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
