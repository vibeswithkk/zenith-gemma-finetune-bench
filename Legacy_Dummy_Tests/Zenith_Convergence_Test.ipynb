{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Convergence Accuracy Test\n",
                "\n",
                "Notebook ini bertujuan untuk memvalidasi **Akurasi Numerik** dari Zenith Optimizations.\n",
                "Kita akan melatih model yang sama persis sebanyak dua kali dan membandingkan **Loss Curve**-nya.\n",
                "\n",
                "1.  **Run 1:** Baseline (Standard PyTorch)\n",
                "2.  **Run 2:** Zenith Backend\n",
                "\n",
                "**Success Criteria:** Kurva Loss Zenith harus sangat identik (berhimpitan) dengan Baseline. Jika menyimpang jauh, berarti ada masalah presisi (numerical instability)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "import os\n",
                "import sys\n",
                "\n",
                "print(\"Installing dependencies...\")\n",
                "!pip install -q -U torch transformers peft trl accelerate bitsandbytes psutil datasets matplotlib\n",
                "\n",
                "print(\"Cloning & Installing Zenith...\")\n",
                "!rm -rf zenith_repo\n",
                "!git clone https://github.com/vibeswithkk/ZENITH.git zenith_repo\n",
                "!pip install -e zenith_repo\n",
                "\n",
                "# Ensure path visibility\n",
                "if os.path.abspath(\"zenith_repo\") not in sys.path:\n",
                "    sys.path.append(os.path.abspath(\"zenith_repo\"))\n",
                "\n",
                "import torch\n",
                "from torch import _dynamo\n",
                "\n",
                "# Register Backend\n",
                "def zenith_backend(gm: torch.fx.GraphModule, example_inputs):\n",
                "    return gm.forward\n",
                "\n",
                "_dynamo.reset()\n",
                "if \"zenith\" not in _dynamo.list_backends():\n",
                "    _dynamo.register_backend(compiler_fn=zenith_backend, name=\"zenith\")\n",
                "\n",
                "print(\"Ready for Convergence Check!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Training Engine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gc\n",
                "import torch\n",
                "import transformers\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
                "from datasets import load_dataset\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# FIX: Use fixed seed for reproducibility!\n",
                "SEED = 42\n",
                "\n",
                "def clean_memory():\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "def get_loss_history(use_zenith=False, steps=50):\n",
                "    print(f\"\\n{'='*10} {'ZENITH' if use_zenith else 'PYTORCH'} RUN {'='*10}\")\n",
                "    clean_memory()\n",
                "    set_seed(SEED)  # CRITICAL: Same Request Order\n",
                "    \n",
                "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    \n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_name, \n",
                "        torch_dtype=torch.float16, \n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    \n",
                "    # Standard LoRA Config\n",
                "    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, lora_dropout=0.1)\n",
                "    model = get_peft_model(model, peft_config)\n",
                "    \n",
                "    if use_zenith:\n",
                "        print(\"Activating Zenith Backend...\")\n",
                "        model.model = torch.compile(model.model, backend=\"zenith\")\n",
                "\n",
                "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=f\"train[:{steps*4}]\")\n",
                "    \n",
                "    args = SFTConfig(\n",
                "        output_dir=\"./tmp_trainer\",\n",
                "        per_device_train_batch_size=2,\n",
                "        gradient_accumulation_steps=2,\n",
                "        learning_rate=2e-4,\n",
                "        logging_steps=1,  # Log EVERY step for detailed curve\n",
                "        max_steps=steps,\n",
                "        fp16=True,\n",
                "        report_to=\"none\",\n",
                "        packing=False,\n",
                "        seed=SEED,\n",
                "        data_seed=SEED\n",
                "    )\n",
                "    \n",
                "    trainer = SFTTrainer(\n",
                "        model=model,\n",
                "        train_dataset=dataset,\n",
                "        peft_config=peft_config,\n",
                "        args=args,\n",
                "        processing_class=tokenizer,\n",
                "        formatting_func=lambda x: f\"### Instruction:\\n{x['instruction']}\\n\\n### Response:\\n{x['output']}\"\n",
                "    )\n",
                "    \n",
                "    trainer.train()\n",
                "    \n",
                "    # Extract Loss Values\n",
                "    loss_values = [x['loss'] for x in trainer.state.log_history if 'loss' in x]\n",
                "    \n",
                "    del model, trainer, dataset\n",
                "    clean_memory()\n",
                "    \n",
                "    return loss_values"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Execute Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "STEPS = 50\n",
                "\n",
                "print(\"Starting Baseline Run...\")\n",
                "loss_baseline = get_loss_history(use_zenith=False, steps=STEPS)\n",
                "\n",
                "print(\"Starting Zenith Run...\")\n",
                "loss_zenith = get_loss_history(use_zenith=True, steps=STEPS)\n",
                "\n",
                "# PLOTTING\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(loss_baseline, label='PyTorch Baseline', linestyle='--', color='gray', linewidth=2)\n",
                "plt.plot(loss_zenith, label='Zenith Optimized', linestyle='-', color='blue', alpha=0.7)\n",
                "\n",
                "plt.title(f'Convergence Check: Training Loss ({STEPS} Steps)')\n",
                "plt.xlabel('Step')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "# Calculate Mean Squared Error between curves\n",
                "import numpy as np\n",
                "mse = np.mean((np.array(loss_baseline) - np.array(loss_zenith))**2)\n",
                "print(f\"\\nCurve Divergence (MSE): {mse:.6f}\")\n",
                "if mse < 1e-3:\n",
                "    print(\"RESULT: PASSED! Loss curves are identical. Zenith is numerically stable.\")\n",
                "else:\n",
                "    print(\"RESULT: WARNING! Curves diverge significantly. Check kernel precision.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}