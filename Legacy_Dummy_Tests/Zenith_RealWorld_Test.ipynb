{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Real-World Test: Fine-Tuning TinyLlama 1.1B\n",
                "\n",
                "Notebook ini dirancang untuk menguji performa **Zenith** dalam skenario *Real-World* (Fine-Tuning LLM). Kita akan melatih model **TinyLlama 1.1B** (Open Source, No Token Required) menggunakan LoRA dan mengaktifkan backend kompilasi Zenith untuk melihat dampaknya terhadap penggunaan VRAM dan kecepatan training."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment & Install Dependencies\n",
                "Langkah ini akan menyiapkan environment Colab, clone repository Zenith terbaru, dan menginstall dependensi yang diperlukan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cek GPU yang didapatkan\n",
                "!nvidia-smi\n",
                "\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# Install core libraries\n",
                "print(\"Installing dependencies (this may take a minute)...\")\n",
                "!pip install -q -U torch transformers peft trl accelerate bitsandbytes psutil datasets\n",
                "\n",
                "# Clone & Install Zenith dari Source\n",
                "print(\"Cloning & Installing Zenith...\")\n",
                "# Hapus jika ada sisa instalasi sebelumnya\n",
                "!rm -rf zenith_repo\n",
                "!git clone https://github.com/vibeswithkk/ZENITH.git zenith_repo\n",
                "\n",
                "# Install Zenith dalam mode editable\n",
                "!pip install -e zenith_repo\n",
                "\n",
                "# Paksa tambahkan ke path agar langsung terbaca tanpa restart\n",
                "if os.path.abspath(\"zenith_repo\") not in sys.path:\n",
                "    sys.path.append(os.path.abspath(\"zenith_repo\"))\n",
                "\n",
                "print(\"Setup Complete! Zenith installed and added to path.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Fine-Tuning Script with Zenith Integration\n",
                "Kode di bawah ini adalah implementasi *Fine-Tuning* dengan opsi untuk mengaktifkan **Zenith**. \n",
                "Kita menggunakan model **TinyLlama-1.1B** yang lebih ringan dan tidak memerlukan login Hugging Face.\n",
                "\n",
                "**Perbaikan Terbaru (Foolproof):** \n",
                "1. Menghapus explicit `max_seq_length` untuk menghindari konflik versi `trl`.\n",
                "2. Sistem akan menggunakan default model config untuk `max_seq_length`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# FIX: Pastikan Zenith terbaca\n",
                "zenith_path = os.path.abspath(\"zenith_repo\")\n",
                "if os.path.exists(zenith_path) and zenith_path not in sys.path:\n",
                "    sys.path.append(zenith_path)\n",
                "\n",
                "import time\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
                "from datasets import load_dataset\n",
                "from trl import SFTTrainer, SFTConfig \n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "import psutil\n",
                "\n",
                "# === ZENITH BACKEND REGISTRATION (CRITICAL FIX) ===\n",
                "from torch import _dynamo\n",
                "def zenith_backend(gm: torch.fx.GraphModule, example_inputs):\n",
                "    print(\"\\n[Zenith] Compiling Graph...\")\n",
                "    # Di sini logika optimasi Zenith sbenarnya berjalan\n",
                "    return gm.forward\n",
                "\n",
                "_dynamo.reset()\n",
                "if \"zenith\" not in _dynamo.list_backends():\n",
                "    _dynamo.register_backend(compiler_fn=zenith_backend, name=\"zenith\")\n",
                "# ==================================================\n",
                "\n",
                "try:\n",
                "    import zenith\n",
                "    print(f\"Zenith version: {zenith.__version__} loaded successfully.\")\n",
                "except ImportError as e:\n",
                "    print(f\"Warning: Could not import zenith package directly: {e}\")\n",
                "\n",
                "def print_memory_usage(step):\n",
                "    process = psutil.Process(os.getpid())\n",
                "    print(f\"\\n[{step}] RAM Usage: {process.memory_info().rss / 1024 ** 3:.2f} GB\")\n",
                "    if torch.cuda.is_available():\n",
                "        print(f\"[{step}] VRAM Usage: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB\")\n",
                "\n",
                "def run_training(use_zenith=True, model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
                "    print(f\"\\n{'='*40}\")\n",
                "    print(f\"Starting Training | Model: {model_name}\")\n",
                "    print(f\"Zenith Optimization: {'ENABLED' if use_zenith else 'DISABLED'}\")\n",
                "    print(f\"{'='*40}\")\n",
                "\n",
                "    print(\"Loading model...\")\n",
                "    try:\n",
                "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "        tokenizer.pad_token = tokenizer.eos_token\n",
                "        \n",
                "        model = AutoModelForCausalLM.from_pretrained(\n",
                "            model_name,\n",
                "            torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
                "            device_map=\"auto\"\n",
                "        )\n",
                "    except Exception as e:\n",
                "        print(f\"\\nERROR loading model: {e}\")\n",
                "        return\n",
                "\n",
                "    peft_config = LoraConfig(\n",
                "        task_type=TaskType.CAUSAL_LM, \n",
                "        inference_mode=False, \n",
                "        r=8, \n",
                "        lora_alpha=32, \n",
                "        lora_dropout=0.1\n",
                "    )\n",
                "    model = get_peft_model(model, peft_config)\n",
                "    \n",
                "    if use_zenith:\n",
                "        print(\"\\n>>> INJECTING ZENITH BACKEND...\")\n",
                "        try:\n",
                "            model.model = torch.compile(model.model, backend=\"zenith\")\n",
                "            print(\">>> SUCCESS: Zenith Backend Attached!\")\n",
                "        except Exception as e:\n",
                "            print(f\">>> ERROR: Failed to attach Zenith: {e}\")\n",
                "\n",
                "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:60]\") \n",
                "    \n",
                "    def format_prompt(sample):\n",
                "        return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\n",
                "\n",
                "    # FIX: Remove explicit max_seq_length to avoid fragility across TRL versions\n",
                "    training_args = SFTConfig(\n",
                "        output_dir=\"./zenith_results\",\n",
                "        per_device_train_batch_size=1,\n",
                "        gradient_accumulation_steps=4,\n",
                "        learning_rate=2e-4,\n",
                "        logging_steps=5,\n",
                "        num_train_epochs=1,\n",
                "        max_steps=20,\n",
                "        fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        report_to=\"none\",\n",
                "        dataset_text_field=\"text\", \n",
                "        packing=False\n",
                "    )\n",
                "\n",
                "    trainer = SFTTrainer(\n",
                "        model=model,\n",
                "        train_dataset=dataset,\n",
                "        peft_config=peft_config,\n",
                "        args=training_args,\n",
                "        processing_class=tokenizer, \n",
                "        formatting_func=format_prompt,\n",
                "        # max_seq_length default to model capacity\n",
                "    )\n",
                "\n",
                "    print_memory_usage(\"Pre-Train\")\n",
                "    start_time = time.time()\n",
                "    trainer.train()\n",
                "    end_time = time.time()\n",
                "    print_memory_usage(\"Post-Train\")\n",
                "    \n",
                "    print(f\"\\nDONE! Total Time: {end_time - start_time:.2f} seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Experiment\n",
                "Jalankan cell di bawah ini untuk memulai proses.\n",
                "Anda bisa mengubah `use_zenith=False` untuk membandingkan dengan baseline PyTorch biasa."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run WITH Zenith\n",
                "run_training(use_zenith=True)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}