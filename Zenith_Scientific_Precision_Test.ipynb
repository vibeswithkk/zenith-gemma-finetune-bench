{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Scientific Test 2: The Bitwise Investigator\n",
                "\n",
                "**Objective:**\n",
                "Verify that Zenith's speedups do not come at the cost of \"Silent Errors\". We will compare the output of specific mathematical operators against PyTorch native execution, bit-by-bit.\n",
                "\n",
                "**Methodology:**\n",
                "1.  Generate random input tensors (including edge cases with large magnitudes).\n",
                "2.  Pass them through standard layers: `Softmax`, `LayerNorm`, `GELU`.\n",
                "3.  Execute once with `torch.compile(backend='zenith')` and once with Native PyTorch.\n",
                "4.  **Metric:** Calculate the **Maximum Absolute Difference** (`abs(zenith - torch).max()`).\n",
                "\n",
                "**Success Criteria:**\n",
                "*   Difference should be negligible (< `1e-5` for FP32/FP16 mixed).\n",
                "*   Zenith must match PyTorch's numerical behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U pyzenith torch numpy\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import zenith\n",
                "import numpy as np\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "def compare_layer(layer_name, layer, input_tensor):\n",
                "    print(f\"\\n--- Testing {layer_name} ---\")\n",
                "    \n",
                "    # 1. Native PyTorch Run\n",
                "    layer.eval()\n",
                "    with torch.no_grad():\n",
                "        out_native = layer(input_tensor)\n",
                "    \n",
                "    # 2. Zenith Run\n",
                "    # We re-create the layer or compile the existing one\n",
                "    # Ideally, we verify component-level compilation\n",
                "    opt_layer = torch.compile(layer, backend=\"zenith\")\n",
                "    \n",
                "    # Warmup\n",
                "    with torch.no_grad():\n",
                "        _ = opt_layer(input_tensor)\n",
                "        \n",
                "    # Actual Run\n",
                "    with torch.no_grad():\n",
                "        out_zenith = opt_layer(input_tensor)\n",
                "    \n",
                "    # 3. Analysis\n",
                "    diff = torch.abs(out_native - out_zenith)\n",
                "    max_diff = diff.max().item()\n",
                "    mean_diff = diff.mean().item()\n",
                "    \n",
                "    print(f\"Max Diff: {max_diff:.9f}\")\n",
                "    print(f\"Mean Diff: {mean_diff:.9f}\")\n",
                "    \n",
                "    if max_diff < 1e-4:\n",
                "        print(f\"PASSED: {layer_name} is precise.\")\n",
                "    else:\n",
                "        print(f\"FAILED: {layer_name} shows significant divergence!\")\n",
                "        \n",
                "    return max_diff"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Case 1: Softmax (Sensitivity Test)\n",
                "Softmax involves exponentials, which are very sensitive to optimization reordering."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_softmax = torch.randn(1024, 1024, device=device, dtype=torch.float32)\n",
                "layer_softmax = nn.Softmax(dim=-1)\n",
                "\n",
                "compare_layer(\"Softmax (FP32)\", layer_softmax, input_softmax)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Case 2: LayerNorm (Fusion Test)\n",
                "LayerNorm is often fused by compilers. Bad fusion leads to wrong variance calculation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_ln = torch.randn(512, 768, device=device, dtype=torch.float32)\n",
                "layer_ln = nn.LayerNorm(768).to(device)\n",
                "\n",
                "compare_layer(\"LayerNorm\", layer_ln, input_ln)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Case 3: GELU (Approximation Test)\n",
                "GELU often uses `tanh` approximation. We need to check if Zenith's approximation matches PyTorch's."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_gelu = torch.randn(1024, 1024, device=device, dtype=torch.float32)\n",
                "layer_gelu = nn.GELU()\n",
                "\n",
                "compare_layer(\"GELU\", layer_gelu, input_gelu)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}