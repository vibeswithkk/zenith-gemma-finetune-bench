{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Scientific Test 1: Gradient Stability\n",
                "\n",
                "**Objective:**\n",
                "Validate that Zenith's optimizations (Graph Capture, Fusion) do not cause numerical instability (Gradient Explosion/Vanishing) during extended training runs.\n",
                "\n",
                "**Methodology:**\n",
                "1.  Train a simple ResNet-like model on dummy data for 1000 steps.\n",
                "2.  Use a relatively high Learning Rate (`1e-3`) to induce stress.\n",
                "3.  **Monitor:** Gradient Norm (`L2 Norm`) of the model parameters at every step.\n",
                "4.  **Comparison:** Zenith vs PyTorch Native.\n",
                "\n",
                "**Success Criteria:**\n",
                "*   No `NaN` or `Inf` values.\n",
                "*   Gradient Norm curve of Zenith should be comparable (smoothness/magnitude) to PyTorch.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment & Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -U pyzenith torch torchvision matplotlib numpy\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import time\n",
                "import zenith\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "# Reproducibility\n",
                "def set_seed(seed=42):\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    np.random.seed(seed)\n",
                "\n",
                "set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model & Synthetic Data\n",
                "We use a synthetic workload to isolate compiler behavior from data noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple ConvNet for stability testing\n",
                "class StabilityNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.features = nn.Sequential(\n",
                "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(64),\n",
                "            nn.ReLU(),\n",
                "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(128),\n",
                "            nn.ReLU(),\n",
                "            nn.AdaptiveAvgPool2d((1, 1))\n",
                "        )\n",
                "        self.classifier = nn.Linear(128, 10)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.features(x)\n",
                "        x = torch.flatten(x, 1)\n",
                "        x = self.classifier(x)\n",
                "        return x\n",
                "\n",
                "# Generate Dummy Data\n",
                "BATCH_SIZE = 128\n",
                "INPUT_SHAPE = (BATCH_SIZE, 3, 32, 32)\n",
                "dummy_input = torch.randn(INPUT_SHAPE, device=device)\n",
                "dummy_target = torch.randint(0, 10, (BATCH_SIZE,), device=device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Stress Test Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_stability_test(use_zenith=False, steps=500):\n",
                "    set_seed(42)\n",
                "    model = StabilityNet().to(device)\n",
                "    optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "    if use_zenith:\n",
                "        print(\"Compiling with Zenith...\")\n",
                "        model = torch.compile(model, backend=\"zenith\")\n",
                "    else:\n",
                "        print(\"Running Native PyTorch...\")\n",
                "\n",
                "    loss_history = []\n",
                "    grad_norm_history = []\n",
                "\n",
                "    start_time = time.time()\n",
                "    \n",
                "    for step in range(steps):\n",
                "        optimizer.zero_grad()\n",
                "        output = model(dummy_input)\n",
                "        loss = criterion(output, dummy_target)\n",
                "        \n",
                "        # Check for Explosion (NaN)\n",
                "        if torch.isnan(loss):\n",
                "            raise ValueError(f\"Loss became NaN at step {step}!\")\n",
                "\n",
                "        loss.backward()\n",
                "        \n",
                "        # CAPTURE GRADIENT NORM\n",
                "        total_norm = 0.0\n",
                "        for p in model.parameters():\n",
                "            if p.grad is not None:\n",
                "                param_norm = p.grad.data.norm(2)\n",
                "                total_norm += param_norm.item() ** 2\n",
                "        total_norm = total_norm ** 0.5\n",
                "        \n",
                "        grad_norm_history.append(total_norm)\n",
                "        loss_history.append(loss.item())\n",
                "        \n",
                "        optimizer.step()\n",
                "\n",
                "    duration = time.time() - start_time\n",
                "    print(f\"Done. Duration: {duration:.4f}s\")\n",
                "    \n",
                "    return loss_history, grad_norm_history, duration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Execute Comparison\n",
                "STEPS = 500\n",
                "\n",
                "print(\"--- Run 1: PyTorch Native ---\")\n",
                "loss_py, grad_py, time_py = run_stability_test(use_zenith=False, steps=STEPS)\n",
                "\n",
                "print(\"\\n--- Run 2: Zenith Optimized ---\")\n",
                "loss_zen, grad_zen, time_zen = run_stability_test(use_zenith=True, steps=STEPS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visual Analysis (The Evidence)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
                "\n",
                "# Plot 1: Loss Curve\n",
                "ax[0].plot(loss_py, label=\"PyTorch\", color=\"gray\", alpha=0.7, linestyle=\"--\")\n",
                "ax[0].plot(loss_zen, label=\"Zenith\", color=\"blue\", linewidth=1.5)\n",
                "ax[0].set_title(\"Training Loss Stability\")\n",
                "ax[0].set_xlabel(\"Step\")\n",
                "ax[0].set_ylabel(\"Loss\")\n",
                "ax[0].legend()\n",
                "ax[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: Gradient Norm (Crucial for exploding gradients)\n",
                "ax[1].plot(grad_py, label=\"PyTorch\", color=\"gray\", alpha=0.7, linestyle=\"--\")\n",
                "ax[1].plot(grad_zen, label=\"Zenith\", color=\"red\", linewidth=1.5)\n",
                "ax[1].set_title(\"Gradient Norm Monitoring\")\n",
                "ax[1].set_xlabel(\"Step\")\n",
                "ax[1].set_ylabel(\"L2 Norm of Gradients\")\n",
                "ax[1].legend()\n",
                "ax[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"zenith_stability_chart.png\")\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nTotal Time Comparison: PyTorch ({time_py:.2f}s) vs Zenith ({time_zen:.2f}s)\")\n",
                "print(f\"Max Gradient Norm: PyTorch ({max(grad_py):.4f}) vs Zenith ({max(grad_zen):.4f})\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}