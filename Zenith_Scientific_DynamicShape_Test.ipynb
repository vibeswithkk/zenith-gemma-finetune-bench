{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith Scientific Test 3: The Dynamic Shape Torture Test\n",
                "\n",
                "**Objective:**\n",
                "Real-world AI workloads (like Chatbots) have variable input lengths. Static compilers often fail here, triggering slow \"re-compilation\" for every new shape.\n",
                "This test verifies if Zenith handles **Dynamic Shapes** gracefully.\n",
                "\n",
                "**Methodology:**\n",
                "1.  Define a Linear Layer model.\n",
                "2.  Run an inference loop where `batch_size` is fixed (32), but `seq_len` changes randomly (between 100 and 1000) **every single iteration**.\n",
                "3.  **Metric:** Measure latency of each step. Watch for \"Spikes\".\n",
                "\n",
                "**Success Criteria:**\n",
                "*   **No compiling pauses:** The first few steps might be slow (warmup), but subsequent steps must be fast regardless of shape changes.\n",
                "*   **Linear Scaling:** Latency should increase linearly with sequence length, not exponentially."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U pyzenith torch numpy matplotlib\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import time\n",
                "import random\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import zenith\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A simple scalable model\n",
                "class DynamicNet(nn.Module):\n",
                "    def __init__(self, hidden_dim=4096):\n",
                "        super().__init__()\n",
                "        self.layer1 = nn.Linear(hidden_dim, hidden_dim)\n",
                "        self.act = nn.GELU()\n",
                "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.layer2(self.act(self.layer1(x)))\n",
                "\n",
                "model = DynamicNet().to(device)\n",
                "# Mark as dynamic? Some compilers need hints. \n",
                "# Zenith aims to be automatic, so we provide no hints."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_dynamic_test(model_fn, iterations=50):\n",
                "    latencies = []\n",
                "    shapes = []\n",
                "    \n",
                "    print(\"Starting Dynamic Loop...\")\n",
                "    for i in range(iterations):\n",
                "        # Random Sequence Length between 128 and 1024\n",
                "        seq_len = random.randint(128, 1024)\n",
                "        batch_size = 32\n",
                "        \n",
                "        x = torch.randn(batch_size, seq_len, 4096, device=device)\n",
                "        \n",
                "        torch.cuda.synchronize()\n",
                "        start = time.time()\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            out = model_fn(x)\n",
                "            \n",
                "        torch.cuda.synchronize()\n",
                "        # Record time in ms\n",
                "        latencies.append((time.time() - start) * 1000)\n",
                "        shapes.append(seq_len)\n",
                "        \n",
                "        if i % 10 == 0:\n",
                "            print(f\"Step {i}: SeqLen={seq_len} -> {latencies[-1]:.2f}ms\")\n",
                "            \n",
                "    return shapes, latencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. PyTorch Eager (Baseline)\n",
                "# Eager mode naturally handles dynamic shapes perfectly.\n",
                "print(\"--- Baseline: PyTorch Eager ---\")\n",
                "shapes, lat_py = run_dynamic_test(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Zenith Optimized\n",
                "print(\"\\n--- Zenith Optimized ---\")\n",
                "# We compile once. \n",
                "# If Zenith supports dynamic shapes, it won't recompile on every new shape.\n",
                "opt_model = torch.compile(model, backend=\"zenith\")\n",
                "\n",
                "# Warmup with a fixed shape\n",
                "dummy = torch.randn(32, 512, 4096, device=device)\n",
                "opt_model(dummy)\n",
                "print(\"Warmup Done.\")\n",
                "\n",
                "_, lat_zen = run_dynamic_test(opt_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(shapes, lat_py, alpha=0.6, label=\"PyTorch Eager\", color=\"gray\")\n",
                "plt.scatter(shapes, lat_zen, alpha=0.8, label=\"Zenith\", color=\"blue\")\n",
                "\n",
                "plt.title(\"Dynamic Shape Performance: SeqLen vs Latency\")\n",
                "plt.xlabel(\"Sequence Length (Input Size)\")\n",
                "plt.ylabel(\"Latency (ms)\")\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.savefig(\"zenith_dynamic_shape.png\")\n",
                "plt.show()\n",
                "\n",
                "print(\"Interpretation: Points should form a clean line. Outliers high above the line indicate re-compilation events.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}