{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith v0.3.0: The Real-World Benchmark\n",
                "\n",
                "This is the **Real World Test** for **Zenith v0.3.0**.\n",
                "Unlike previous tests that used a *dummy backend*, this notebook will:\n",
                "1.  Install the actual `pyzenith` library.\n",
                "2.  Let Zenith handle backend registration internally.\n",
                "3.  Measure the pure performance of the kernel optimizations present in v0.3.0.\n",
                "\n",
                "**Objective:**\n",
                "*   **Training:** Fine-Tune TinyLlama (Alpaca Dataset) - Speed & VRAM.\n",
                "*   **Inference:** Text Generation - Tokens Per Second (TPS).\n",
                "\n",
                "**Hardware Recom:** T4 GPU (Google Colab Standard)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "\n",
                "print(\"Installing dependencies (Transformers, PEFT, TRL)...\")\n",
                "!pip install -q -U torch transformers peft trl accelerate bitsandbytes psutil datasets matplotlib\n",
                "\n",
                "print(\"Installing ZENITH v0.3.0...\")\n",
                "# Option 1: Install from PyPI (If available)\n",
                "!pip install -U pyzenith\n",
                "\n",
                "# Option 2: Fallback to Github (If PyPI is pending)\n",
                "# !pip install git+https://github.com/vibeswithkk/ZENITH.git\n",
                "\n",
                "import torch\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Verify Zenith Integration (The Moment of Truth)\n",
                "Here we check if `import zenith` successfully registers the backend automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import zenith\n",
                "from torch import _dynamo\n",
                "\n",
                "print(\"Checking Registered Backends...\")\n",
                "backends = _dynamo.list_backends()\n",
                "print(f\"Available Backends: {backends}\")\n",
                "\n",
                "if \"zenith\" in backends:\n",
                "    print(\"SUCCESS: Zenith Backend is registered natively!\")\n",
                "else:\n",
                "    print(\"WARNING: Zenith Backend NOT found. Did v0.3.0 install correctly?\")\n",
                "    # Stop execution if Zenith is missing in a Real World test\n",
                "    raise RuntimeError(\"Zenith Backend not found! Cannot proceed with Real World Benchmark.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Benchmark (Fine-Tuning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import gc\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
                "from datasets import load_dataset\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "SEED = 42\n",
                "STEPS = 50\n",
                "\n",
                "def clean_memory():\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "def run_training(use_zenith=False):\n",
                "    mode = \"ZENITH\" if use_zenith else \"PYTORCH\"\n",
                "    print(f\"\\n{'='*10} STARTING: {mode} {'='*10}\")\n",
                "    clean_memory()\n",
                "    set_seed(SEED)\n",
                "    \n",
                "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    \n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_name, \n",
                "        torch_dtype=torch.float16, \n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    \n",
                "    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, lora_dropout=0.1)\n",
                "    model = get_peft_model(model, peft_config)\n",
                "    \n",
                "    # === THE REAL WORLD INTEGRATION ===\n",
                "    # No dummy functions here. Just pure torch.compile.\n",
                "    if use_zenith:\n",
                "        print(\"Activating Zenith Optimization (Native)...\")\n",
                "        model.model = torch.compile(model.model, backend=\"zenith\")\n",
                "    # ==================================\n",
                "\n",
                "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=f\"train[:{STEPS*4}]\")\n",
                "    \n",
                "    args = SFTConfig(\n",
                "        output_dir=f\"./tmp_{mode}\",\n",
                "        per_device_train_batch_size=2,\n",
                "        gradient_accumulation_steps=2,\n",
                "        learning_rate=2e-4,\n",
                "        max_steps=STEPS,\n",
                "        fp16=True,\n",
                "        report_to=\"none\",\n",
                "        packing=False,\n",
                "    )\n",
                "    \n",
                "    trainer = SFTTrainer(\n",
                "        model=model,\n",
                "        train_dataset=dataset,\n",
                "        peft_config=peft_config,\n",
                "        args=args,\n",
                "        processing_class=tokenizer,\n",
                "        formatting_func=lambda x: f\"### Instruction:\\n{x['instruction']}\\n\\n### Response:\\n{x['output']}\"\n",
                "    )\n",
                "    \n",
                "    torch.cuda.synchronize()\n",
                "    start_t = time.time()\n",
                "    trainer.train()\n",
                "    torch.cuda.synchronize()\n",
                "    end_t = time.time()\n",
                "    \n",
                "    total_time = end_t - start_t\n",
                "    max_mem = torch.cuda.max_memory_allocated() / 1e9\n",
                "    \n",
                "    print(f\"DONE {mode}. Time: {total_time:.2f}s | VRAM: {max_mem:.2f}GB\")\n",
                "    \n",
                "    del model, trainer, dataset\n",
                "    clean_memory()\n",
                "    return total_time, max_mem"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Benchmarks\n",
                "time_py, mem_py = run_training(use_zenith=False)\n",
                "time_zen, mem_zen = run_training(use_zenith=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Inference Benchmark (TPS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def run_inference(use_zenith=False, runs=5):\n",
                "    mode = \"ZENITH\" if use_zenith else \"PYTORCH\"\n",
                "    print(f\"\\n{'='*10} INFERENCE: {mode} {'='*10}\")\n",
                "    clean_memory()\n",
                "    \n",
                "    tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
                "    model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16, device_map=\"cuda\")\n",
                "    \n",
                "    if use_zenith:\n",
                "        print(\"Compiling with Zenith...\")\n",
                "        model = torch.compile(model, backend=\"zenith\")\n",
                "        \n",
                "    input_ids = tokenizer(\"The future of AI is\", return_tensors=\"pt\").input_ids.cuda()\n",
                "    \n",
                "    # Warmup\n",
                "    model.generate(input_ids, max_new_tokens=10)\n",
                "    \n",
                "    tps_list = []\n",
                "    for _ in range(runs):\n",
                "        torch.cuda.synchronize()\n",
                "        start = time.time()\n",
                "        out = model.generate(input_ids, max_new_tokens=100)\n",
                "        torch.cuda.synchronize()\n",
                "        lat = time.time() - start\n",
                "        tps = 100 / lat\n",
                "        tps_list.append(tps)\n",
                "        print(f\"Run: {tps:.2f} TPS\")\n",
                "        \n",
                "    return np.mean(tps_list)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tps_py = run_inference(use_zenith=False)\n",
                "tps_zen = run_inference(use_zenith=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\n{'='*40}\")\n",
                "print(\"ZENITH v0.3.0 REAL WORLD RESULTS\")\n",
                "print(f\"{'='*40}\")\n",
                "print(f\"Training Time : PyTorch {time_py:.2f}s | Zenith {time_zen:.2f}s | Speedup: {((time_py-time_zen)/time_py)*100:+.2f}%\")\n",
                "print(f\"Inference TPS : PyTorch {tps_py:.2f}  | Zenith {tps_zen:.2f}  | Speedup: {((tps_zen-tps_py)/tps_py)*100:+.2f}%\")\n",
                "\n",
                "# Simple Bar Chart\n",
                "labels = ['Training Time (s)', 'Inference Speed (TPS)']\n",
                "py_vals = [time_py, tps_py]\n",
                "zen_vals = [time_zen, tps_zen]\n",
                "\n",
                "x = np.arange(len(labels))\n",
                "width = 0.35\n",
                "\n",
                "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Chart 1: Training Time (Lower is better)\n",
                "ax[0].bar(['PyTorch', 'Zenith'], [time_py, time_zen], color=['gray', 'blue'])\n",
                "ax[0].set_title('Training Time (Lower is Better)')\n",
                "ax[0].set_ylabel('Seconds')\n",
                "\n",
                "# Chart 2: Inference TPS (Higher is better)\n",
                "ax[1].bar(['PyTorch', 'Zenith'], [tps_py, tps_zen], color=['gray', 'orange'])\n",
                "ax[1].set_title('Inference TPS (Higher is Better)')\n",
                "ax[1].set_ylabel('Tokens / Sec')\n",
                "\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}